{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8hS/YMEm3Sh4hl4hgP2MR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhilp90/Hack4Humanity/blob/main/RecommendationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TPybAq7YWlq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Step 1: Load Data\n",
        "def load_data(job_file, user_file):\n",
        "    \"\"\"\n",
        "    Load job and user data from CSV files.\n",
        "    \"\"\"\n",
        "    jobs = pd.read_csv(job_file)\n",
        "    users = pd.read_csv(user_file)\n",
        "    return jobs, users\n",
        "\n",
        "# Step 2: Enhanced Feature Engineering\n",
        "def create_weighted_features(jobs_df):\n",
        "    \"\"\"\n",
        "    Create a weighted combined feature string for each job.\n",
        "    Skills are given the highest weight, followed by location, company, job type, and experience level.\n",
        "    \"\"\"\n",
        "    jobs_df[\"weighted_features\"] = (\n",
        "        \" \".join([jobs_df[\"skills\"]] * 3) + \" \" +\n",
        "        \" \".join([jobs_df[\"location\"]] * 2) + \" \" +\n",
        "        \" \".join([jobs_df[\"company\"]] * 2) + \" \" +\n",
        "        \" \".join([jobs_df[\"job_type\"]] * 2) + \" \" +\n",
        "        jobs_df[\"experience_level\"]\n",
        "    )\n",
        "    return jobs_df\n",
        "\n",
        "# Step 3: Hybrid Recommendation System with TF-IDF and User Interaction Data\n",
        "def recommend_jobs(jobs_df, user_skills, user_location, user_experience, user_interactions=None, top_n=5):\n",
        "    \"\"\"\n",
        "    Recommend jobs based on cosine similarity between the user profile and job features.\n",
        "    Uses both content-based and collaborative filtering (if user interaction data is provided).\n",
        "    \"\"\"\n",
        "\n",
        "    # Create user profile with weighted preferences\n",
        "    user_profile = (\n",
        "        \" \".join([user_skills] * 3) + \" \" +\n",
        "        \" \".join([user_location] * 2) + \" \" +\n",
        "        \" \".join([user_interactions]) + \" \" +\n",
        "        user_experience\n",
        "    )\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(\n",
        "        pd.concat([jobs_df[\"weighted_features\"], pd.Series(user_profile)])\n",
        "    )\n",
        "\n",
        "    # Compute cosine similarity between user profile and job postings\n",
        "    similarity = cosine_similarity(vectors[-1], vectors[:-1])\n",
        "    jobs_df[\"similarity\"] = similarity.flatten()\n",
        "\n",
        "    # If user interaction data (ratings or clicks) is provided, use collaborative filtering to enhance the recommendation\n",
        "    if user_interactions is not None:\n",
        "        # Here, a simple collaborative filtering matrix factorization (SVD) can be used for enhancing recommendations\n",
        "        user_interaction_matrix = user_interactions.pivot(index='user_id', columns='job_id', values='interaction')\n",
        "        svd = TruncatedSVD(n_components=20)\n",
        "        user_latent_factors = svd.fit_transform(user_interaction_matrix.fillna(0))\n",
        "\n",
        "        # Cosine similarity on latent factors for collaborative filtering\n",
        "        cf_similarity = cosine_similarity(user_latent_factors, svd.components_)\n",
        "        jobs_df[\"collaborative_similarity\"] = cf_similarity.flatten()\n",
        "\n",
        "        # Combine content-based similarity with collaborative filtering similarity\n",
        "        jobs_df[\"final_similarity\"] = (jobs_df[\"similarity\"] + jobs_df[\"collaborative_similarity\"]) / 2\n",
        "    else:\n",
        "        jobs_df[\"final_similarity\"] = jobs_df[\"similarity\"]\n",
        "\n",
        "    # Return top N job recommendations based on final similarity\n",
        "    recommendations = jobs_df.sort_values(by=\"final_similarity\", ascending=False).head(top_n)\n",
        "    return recommendations[[\"job_title\", \"skills\", \"location\", \"similarity\", \"final_similarity\"]]\n",
        "\n",
        "# Step 4: Model Training and Evaluation (for hybrid recommendation)\n",
        "def train_and_evaluate(jobs_df, user_df):\n",
        "    \"\"\"\n",
        "    Train and evaluate the recommendation model.\n",
        "    Uses a hybrid approach (content-based + collaborative filtering).\n",
        "    \"\"\"\n",
        "    # Use train-test split to evaluate the model\n",
        "    train, test = train_test_split(user_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model on training data\n",
        "    model_recommendations = []\n",
        "    for _, user in train.iterrows():\n",
        "        recommendations = recommend_jobs(\n",
        "            jobs_df,\n",
        "            user[\"skills\"],\n",
        "            user[\"preferred_location\"],\n",
        "            user[\"experience_level\"],\n",
        "            user_interactions=train,\n",
        "            top_n=5\n",
        "        )\n",
        "        model_recommendations.append(recommendations)\n",
        "\n",
        "    # Evaluate the model using MSE or any other metric\n",
        "    y_true = test[\"interaction\"].values\n",
        "    y_pred = [rec[\"final_similarity\"].mean() for rec in model_recommendations]\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    print(f\"Model Evaluation (MSE): {mse}\")\n",
        "\n",
        "# Step 5: Main Execution Flow\n",
        "def main():\n",
        "    # Load data\n",
        "    jobs_df, users_df = load_data(\"jobs.csv\", \"users.csv\")\n",
        "\n",
        "    # Engineer features\n",
        "    jobs_df = create_weighted_features(jobs_df)\n",
        "\n",
        "    # Select a user (first user in this example)\n",
        "    user = users_df.iloc[0]\n",
        "    user_skills = user[\"skills\"]\n",
        "    user_location = user[\"preferred_location\"]\n",
        "    user_experience = user[\"experience_level\"]\n",
        "\n",
        "    # Generate recommendations\n",
        "    recommendations = recommend_jobs(jobs_df, user_skills, user_location, user_experience, top_n=5)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Job Recommendations for {user['name']}:\\n\")\n",
        "    print(recommendations.to_string(index=False))\n",
        "\n",
        "    # Optionally, train and evaluate the model (if interactions data is provided)\n",
        "    train_and_evaluate(jobs_df, users_df)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}